"""
Metrics Collection and Alerting System

Provides centralized metrics collection, aggregation, and alerting for
uncertainty calculation pipelines in production deployments.
"""

import time
import threading
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, field
from collections import defaultdict, deque
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class Alert:
    """Alert generated by metrics monitoring"""
    id: str
    severity: AlertSeverity
    title: str
    description: str
    timestamp: float
    metric_name: str
    metric_value: float
    threshold: float
    additional_context: Dict[str, Any] = field(default_factory=dict)


@dataclass
class MetricSummary:
    """Summary statistics for a metric"""
    name: str
    count: int
    mean: float
    std: float
    min: float
    max: float
    p50: float
    p95: float
    p99: float
    last_updated: float


class UncertaintyMetricsCollector:
    """Collects and stores uncertainty-related metrics"""

    def __init__(self,
                 max_history: int = 10000,
                 retention_hours: int = 24):
        self.max_history = max_history
        self.retention_seconds = retention_hours * 3600

        # Metrics storage
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_history))
        self.timestamps: Dict[str, deque] = defaultdict(lambda: deque(maxlen=max_history))

        # Thread safety
        self._lock = threading.RLock()

        # Background cleanup
        self._cleanup_thread = threading.Thread(target=self._cleanup_old_metrics, daemon=True)
        self._cleanup_thread.start()

    def record_metric(self, name: str, value: float, timestamp: Optional[float] = None) -> None:
        """Record a metric value"""
        if timestamp is None:
            timestamp = time.time()

        with self._lock:
            self.metrics[name].append(value)
            self.timestamps[name].append(timestamp)

    def get_metric_summary(self, name: str, window_seconds: Optional[float] = None) -> Optional[MetricSummary]:
        """Get summary statistics for a metric"""
        with self._lock:
            if name not in self.metrics or len(self.metrics[name]) == 0:
                return None

            values = list(self.metrics[name])
            timestamps = list(self.timestamps[name])

            # Apply time window filter if specified
            if window_seconds is not None:
                cutoff_time = time.time() - window_seconds
                filtered_data = [(v, t) for v, t in zip(values, timestamps) if t >= cutoff_time]

                if not filtered_data:
                    return None

                values, timestamps = zip(*filtered_data)
                values = list(values)

            # Calculate statistics
            import numpy as np

            return MetricSummary(
                name=name,
                count=len(values),
                mean=float(np.mean(values)),
                std=float(np.std(values)),
                min=float(np.min(values)),
                max=float(np.max(values)),
                p50=float(np.percentile(values, 50)),
                p95=float(np.percentile(values, 95)),
                p99=float(np.percentile(values, 99)),
                last_updated=max(timestamps) if timestamps else 0.0
            )

    def get_all_metrics(self) -> List[str]:
        """Get list of all recorded metric names"""
        with self._lock:
            return list(self.metrics.keys())

    def _cleanup_old_metrics(self) -> None:
        """Background cleanup of old metrics"""
        while True:
            try:
                current_time = time.time()
                cutoff_time = current_time - self.retention_seconds

                with self._lock:
                    for metric_name in list(self.metrics.keys()):
                        # Remove old entries
                        timestamps = self.timestamps[metric_name]
                        values = self.metrics[metric_name]

                        # Find entries to keep
                        keep_indices = [i for i, ts in enumerate(timestamps) if ts >= cutoff_time]

                        if len(keep_indices) < len(timestamps):
                            # Rebuild deques with only recent entries
                            new_values = deque([values[i] for i in keep_indices], maxlen=self.max_history)
                            new_timestamps = deque([timestamps[i] for i in keep_indices], maxlen=self.max_history)

                            self.metrics[metric_name] = new_values
                            self.timestamps[metric_name] = new_timestamps

                # Sleep for 5 minutes before next cleanup
                time.sleep(300)

            except Exception as e:
                logger.warning(f"Error in metrics cleanup: {e}")
                time.sleep(60)


class MetricsAggregator:
    """Aggregates metrics across multiple collectors"""

    def __init__(self):
        self.collectors: Dict[str, UncertaintyMetricsCollector] = {}
        self._lock = threading.RLock()

    def add_collector(self, name: str, collector: UncertaintyMetricsCollector) -> None:
        """Add a metrics collector"""
        with self._lock:
            self.collectors[name] = collector

    def remove_collector(self, name: str) -> None:
        """Remove a metrics collector"""
        with self._lock:
            if name in self.collectors:
                del self.collectors[name]

    def get_aggregated_summary(self,
                             metric_name: str,
                             window_seconds: Optional[float] = None) -> Optional[MetricSummary]:
        """Get aggregated summary across all collectors"""
        with self._lock:
            all_values = []
            all_timestamps = []

            for collector in self.collectors.values():
                if metric_name in collector.metrics:
                    values = list(collector.metrics[metric_name])
                    timestamps = list(collector.timestamps[metric_name])

                    # Apply time window if specified
                    if window_seconds is not None:
                        cutoff_time = time.time() - window_seconds
                        filtered_data = [(v, t) for v, t in zip(values, timestamps) if t >= cutoff_time]

                        if filtered_data:
                            v, t = zip(*filtered_data)
                            values, timestamps = list(v), list(t)
                        else:
                            values, timestamps = [], []

                    all_values.extend(values)
                    all_timestamps.extend(timestamps)

            if not all_values:
                return None

            # Calculate aggregated statistics
            import numpy as np

            return MetricSummary(
                name=f"aggregated_{metric_name}",
                count=len(all_values),
                mean=float(np.mean(all_values)),
                std=float(np.std(all_values)),
                min=float(np.min(all_values)),
                max=float(np.max(all_values)),
                p50=float(np.percentile(all_values, 50)),
                p95=float(np.percentile(all_values, 95)),
                p99=float(np.percentile(all_values, 99)),
                last_updated=max(all_timestamps) if all_timestamps else 0.0
            )

    def get_all_metric_names(self) -> List[str]:
        """Get all unique metric names across collectors"""
        with self._lock:
            all_names = set()
            for collector in self.collectors.values():
                all_names.update(collector.get_all_metrics())
            return sorted(list(all_names))


class AlertManager:
    """Manages alerts based on metric thresholds"""

    def __init__(self):
        self.alert_rules: Dict[str, Dict[str, Any]] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: deque = deque(maxlen=1000)
        self.alert_handlers: List[Callable[[Alert], None]] = []
        self._lock = threading.RLock()

    def add_alert_rule(self,
                      metric_name: str,
                      threshold: float,
                      severity: AlertSeverity,
                      comparison: str = "greater",
                      window_seconds: Optional[float] = None,
                      cooldown_seconds: float = 300.0) -> None:
        """
        Add an alert rule for a metric.

        Args:
            metric_name: Name of metric to monitor
            threshold: Threshold value for alerting
            severity: Alert severity level
            comparison: 'greater', 'less', 'equal'
            window_seconds: Time window for metric evaluation
            cooldown_seconds: Minimum time between alerts for same rule
        """
        with self._lock:
            rule_id = f"{metric_name}_{comparison}_{threshold}"
            self.alert_rules[rule_id] = {
                "metric_name": metric_name,
                "threshold": threshold,
                "severity": severity,
                "comparison": comparison,
                "window_seconds": window_seconds,
                "cooldown_seconds": cooldown_seconds,
                "last_alert_time": 0.0
            }

    def add_alert_handler(self, handler: Callable[[Alert], None]) -> None:
        """Add alert handler function"""
        with self._lock:
            self.alert_handlers.append(handler)

    def check_metrics(self, aggregator: MetricsAggregator) -> List[Alert]:
        """Check metrics against alert rules and generate alerts"""
        new_alerts = []
        current_time = time.time()

        with self._lock:
            for rule_id, rule in self.alert_rules.items():
                # Check cooldown
                if current_time - rule["last_alert_time"] < rule["cooldown_seconds"]:
                    continue

                # Get metric summary
                summary = aggregator.get_aggregated_summary(
                    rule["metric_name"],
                    rule["window_seconds"]
                )

                if summary is None:
                    continue

                # Check threshold
                metric_value = self._get_comparison_value(summary, rule["comparison"])
                should_alert = self._evaluate_threshold(
                    metric_value,
                    rule["threshold"],
                    rule["comparison"]
                )

                if should_alert:
                    # Create alert
                    alert = Alert(
                        id=f"{rule_id}_{int(current_time)}",
                        severity=rule["severity"],
                        title=f"{rule['metric_name']} threshold exceeded",
                        description=f"{rule['metric_name']} {rule['comparison']} {rule['threshold']} (current: {metric_value:.4f})",
                        timestamp=current_time,
                        metric_name=rule["metric_name"],
                        metric_value=metric_value,
                        threshold=rule["threshold"],
                        additional_context={
                            "rule_id": rule_id,
                            "metric_summary": summary.__dict__,
                            "comparison": rule["comparison"],
                            "window_seconds": rule["window_seconds"]
                        }
                    )

                    # Store and track alert
                    new_alerts.append(alert)
                    self.active_alerts[alert.id] = alert
                    self.alert_history.append(alert)
                    rule["last_alert_time"] = current_time

                    # Notify handlers
                    for handler in self.alert_handlers:
                        try:
                            handler(alert)
                        except Exception as e:
                            logger.warning(f"Alert handler error: {e}")

        return new_alerts

    def _get_comparison_value(self, summary: MetricSummary, comparison: str) -> float:
        """Get the appropriate value from summary for comparison"""
        if comparison in ["greater", "less"]:
            return summary.p95  # Use 95th percentile for threshold comparisons
        else:
            return summary.mean

    def _evaluate_threshold(self, value: float, threshold: float, comparison: str) -> bool:
        """Evaluate if threshold condition is met"""
        if comparison == "greater":
            return value > threshold
        elif comparison == "less":
            return value < threshold
        elif comparison == "equal":
            return abs(value - threshold) < 0.001  # Floating point equality
        else:
            return False

    def get_active_alerts(self, severity: Optional[AlertSeverity] = None) -> List[Alert]:
        """Get currently active alerts"""
        with self._lock:
            alerts = list(self.active_alerts.values())

            if severity is not None:
                alerts = [a for a in alerts if a.severity == severity]

            return sorted(alerts, key=lambda x: x.timestamp, reverse=True)

    def acknowledge_alert(self, alert_id: str) -> bool:
        """Acknowledge and remove an active alert"""
        with self._lock:
            if alert_id in self.active_alerts:
                del self.active_alerts[alert_id]
                return True
            return False

    def get_alert_summary(self) -> Dict[str, int]:
        """Get summary of active alerts by severity"""
        with self._lock:
            summary = {severity.value: 0 for severity in AlertSeverity}

            for alert in self.active_alerts.values():
                summary[alert.severity.value] += 1

            return summary


# Default alert handlers
def console_alert_handler(alert: Alert) -> None:
    """Simple console alert handler"""
    print(f"[{alert.severity.value.upper()}] {alert.title}: {alert.description}")


def logging_alert_handler(alert: Alert) -> None:
    """Logging-based alert handler"""
    log_level = {
        AlertSeverity.INFO: logging.INFO,
        AlertSeverity.WARNING: logging.WARNING,
        AlertSeverity.ERROR: logging.ERROR,
        AlertSeverity.CRITICAL: logging.CRITICAL
    }.get(alert.severity, logging.WARNING)

    logger.log(log_level, f"Alert: {alert.title} - {alert.description}")


# Factory functions for common setups
def create_standard_metrics_setup() -> Tuple[MetricsAggregator, AlertManager]:
    """Create standard metrics aggregator and alert manager with common rules"""
    aggregator = MetricsAggregator()
    alert_manager = AlertManager()

    # Add common alert rules
    alert_manager.add_alert_rule(
        metric_name="uncertainty",
        threshold=0.9,
        severity=AlertSeverity.WARNING,
        comparison="greater",
        window_seconds=300  # 5 minutes
    )

    alert_manager.add_alert_rule(
        metric_name="execution_time",
        threshold=1.0,  # 1 second
        severity=AlertSeverity.WARNING,
        comparison="greater",
        window_seconds=300
    )

    alert_manager.add_alert_rule(
        metric_name="calibration_quality",
        threshold=0.3,  # High ECE indicates poor calibration
        severity=AlertSeverity.ERROR,
        comparison="greater",
        window_seconds=600  # 10 minutes
    )

    # Add default handlers
    alert_manager.add_alert_handler(logging_alert_handler)

    return aggregator, alert_manager